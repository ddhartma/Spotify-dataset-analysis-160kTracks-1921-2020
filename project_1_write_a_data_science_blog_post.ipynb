{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spotify Dataset 1921-2020, 160k+ Tracks\n",
    "\n",
    "https://www.kaggle.com/yamaerenay/spotify-dataset-19212020-160k-tracks\n",
    "\n",
    "CRIPS ANALYSIS:\n",
    "- In order to write a popular song, is the key and mode important?\n",
    "- Is there a change/evolution in the usage of the most popular keys in the last 100 years? \n",
    "- What should I do to become a famous music composer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame SUMMARY: \n",
    "- Dataset with 170653 observations, 19 columns (13 numerical and 6 categorical/dummy) and no nan values\n",
    "- **numerical** columns: \n",
    "    - acousticness: The relative metric of the track being acoustic, (Ranges from 0 to 1)\n",
    "    - danceability: The relative measurement of the track being danceable, (Ranges from 0 to 1)\n",
    "    - energy: The energy of the track,  (Ranges from 0 to 1)\n",
    "    - duration_ms: The length of the track in milliseconds (ms), (Integer typically ranging from 200k to 300k)\n",
    "    - instrumentalness:, The relative ratio of the track being instrumental, (Ranges from 0 to 1)\n",
    "    - valence: The positiveness of the track, (Ranges from 0 to 1)\n",
    "    - popularity: The popularity of the song lately, default country = US, (Ranges from 0 to 100)\n",
    "    - tempo:The tempo of the track in Beat Per Minute (BPM), (Float typically ranging from 50 to 150)\n",
    "    - liveness: The relative duration of the track sounding as a live performance, (Ranges from 0 to 1)\n",
    "    - loudness: Relative loudness of the track in decibel (dB), (Float typically ranging from -60 to 0)\n",
    "    - speechiness: The relative length of the track containing any kind of human voice, (Ranges from 0 to 1)\n",
    "    - year: The release year of track, (Ranges from 1921 to 2020)\n",
    "    - id, The primary identifier for the track, generated by Spotify\n",
    "    \n",
    "- **categorical** columns:\n",
    "    - key: The primary key of the track encoded as integers in between 0 and 11 (starting on C as 0, C# as 1 and so on…)\n",
    "    - artists: The list of artists credited for production of the track \n",
    "    - release_date: Date of release mostly in yyyy-mm-dd format, however precision of date may vary\n",
    "    - name: The title of the track \n",
    "    - mode: The binary value representing whether the track starts with a major (1) chord progression or a minor (0)\n",
    "    - explicit: The binary value whether the track contains explicit content or not, (0 = No explicit content, 1 = Explicit content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content\n",
    "\n",
    "### Anlysis of the Dataframe \n",
    "Shape, descriptive statistics, number of missing values, number and sort of categorical and numerical values \n",
    "##### <a href='#df'>DataFrame Table</a>\n",
    "##### <a href='#df_ds'>Descriptive Statistics</a>\n",
    "##### <a href='#df_sum'>Dtypes, Missing Values, Shape</a>\n",
    "\n",
    "### Cleaning \n",
    "In order to implement a good model for songs popularity some claening steps have to be done\n",
    "##### <a href='#clean_pop'>Missclassified values in popularity</a>\n",
    "\n",
    "\n",
    "### Create dummy variables for categorical variables \n",
    "The 'key' column for example is a column with categorical values. As its dependence on song popularity will be studied,a categorical transformation is needed.\n",
    "### <a href='#dum'>Create dummy variables</a>\n",
    "\n",
    "\n",
    "### Create subsets\n",
    "Time dependent analysis will be done. For this reason the dataset is divided in decades and the decade 2011 to 2020 is also separeted into years.\n",
    "### <a href='#sub'>Create subsets</a>\n",
    "\n",
    "### Evaluation of distributions\n",
    "Evaluation of 'key' and 'mode' frequency distributions for certain time periods. Main question: Does the song popularity dependent on its key scale and/or mode?\n",
    "### <a href='#vc'>Value Counts</a>\n",
    "##### <a href='#vc_h'>Histograms</a>\n",
    "##### <a href='#vc_pd'>Bar Plots & Pie Charts - Key and Mode Distributions 1921-2020</a>\n",
    "##### <a href='#vc_pp'>Pie Charts - Popularity triggered by Key and Mode 1921 -2020?</a>\n",
    "\n",
    "##### <a href='#vc_pd_2011'>Bar Plots & Pie Charts - Key and Mode Distributions 2011-2020</a>\n",
    "##### <a href='#vc_pp_2011'>Pie Charts - Popularity triggered by Key and Mode 2011 -2020?</a>\n",
    "\n",
    "### Correlation analysis\n",
    "What are the main indicators for song popularity?\n",
    "### <a href='#cor'>Correlations</a>\n",
    "##### <a href='#cor_p'>Correlation plot</a>\n",
    "##### <a href='#cor_s'>Correlation sorted</a>\n",
    "\n",
    "### <a href='#cor'>Time Series plots</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Libraries - Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'plot_df'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-3bb8063c20ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_style\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"darkgrid\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mplot_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;31m#matplotlib.use('nbagg')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'plot_df'"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "import matplotlib\n",
    "import plot_df\n",
    "#matplotlib.use('nbagg')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import Markdown, display, HTML\n",
    "\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "\n",
    "# SKLEARN - ML\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# TENSORFLOW - ML\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "df = pd.read_csv('./Spotify_Music_Datasets_1921_2020_data.csv')\n",
    "\n",
    "def plot_bar(df, title, **kwargs):\n",
    "    bar_setting_dict={\n",
    "                    'x_label' : '',\n",
    "                    'y_label' : '',\n",
    "                    'figsize' :(6,4),\n",
    "                    'layout' : (1, 1),\n",
    "                    'width' : 0.3,\n",
    "                    'align' : 'center',\n",
    "                    'subplots' : False,\n",
    "                    'fontsize_title' : 14,\n",
    "                    'fontsize_axes_values' : 14,\n",
    "                    'fontsize_axes_label' : 14,\n",
    "                    'fontsize_text' : 14,\n",
    "                    'fontsize_legend' : 14,\n",
    "                    'set_yticks_range' : False,\n",
    "                    'yticks_start' : None,\n",
    "                    'yticks_end' : None,\n",
    "                    'yticks_step' : None,\n",
    "                    'legend_state' : False,\n",
    "                    'legend_list_to_plot' : '',\n",
    "                    'legend_move' : False,\n",
    "                    'legend_x' : None,\n",
    "                    'legend_y' : None}\n",
    "    for key, value in kwargs.items():\n",
    "        bar_setting_dict[key] = value\n",
    "    plot_df.plot_df_bar(df, title, bar_setting_dict)\n",
    "\n",
    "    \n",
    "def plot_pie(df, title, explode, **kwargs):\n",
    "    pie_chart_settings = {'figsize' : (10,5),\n",
    "                        'shadow' : True,\n",
    "                        'autopct' : '%1.1f%%',\n",
    "                        'startangle' :90,\n",
    "                        'fontsize_title' : 14,\n",
    "                        'fontsize_text' : 14,\n",
    "                        'fontsize_legend' : 14,\n",
    "                        'legend_state' : True,\n",
    "                        'legend_title' : '',   \n",
    "                        'legend_list_to_plot' : '',\n",
    "                        'legend_move' : False,\n",
    "                        'legend_x' : None,\n",
    "                        'legend_y' : None}\n",
    "    \n",
    "    for key, value in kwargs.items():\n",
    "        pie_chart_settings[key] = value\n",
    "    plot_df.plot_df_pie(df, title, explode, pie_chart_settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='df'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='df_ds'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include='all').T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='df_sum'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframe_summary(df, cat_list, dummy_list):  \n",
    "    number_nan = df.isnull().sum()\n",
    "    printmd('### DataFrame Summary')\n",
    "    printmd('- Dataset with {} observations and {} columns'.format(df.shape[0], df.shape[1]))\n",
    "    \n",
    "    print('___________________________________')\n",
    "    printmd('- **Numerical** columns:')\n",
    "    row_num = []\n",
    "    count_int = 0\n",
    "    count_float = 0\n",
    "    for col in df.columns.tolist():\n",
    "        if (df[col].dtypes == 'float64' or df[col].dtypes == 'int64') and col not in cat_list and col not in dummy_list:\n",
    "            row_num.append([col, df[col].dtypes, df[col].min(), df[col].max(), number_nan[col]])\n",
    "            if df[col].dtypes == 'int64':\n",
    "                count_int += 1\n",
    "            if df[col].dtypes == 'float64':\n",
    "                count_float += 1\n",
    "    df_num = pd.DataFrame(row_num,columns=['column_name', 'type', 'min', 'max', 'number NaN'])\n",
    "    display(df_num)\n",
    "\n",
    "    print('___________________________________')\n",
    "    printmd('- **Categorical** columns:')\n",
    "    row_cat = []\n",
    "    count_object = 0\n",
    "    for col in df.columns.tolist():\n",
    "        if df[col].dtypes == 'object' or col in cat_list:\n",
    "            row_cat.append([col, df[col].dtypes, df[col].min(), df[col].max(), number_nan[col]])\n",
    "            count_object += 1\n",
    "    df_cat = pd.DataFrame(row_cat,columns=['column_name', 'type', 'min', 'max', 'number NaN'])\n",
    "    display(df_cat)\n",
    "\n",
    "    print('___________________________________')\n",
    "    printmd('- **Dummy** columns:')\n",
    "    row_dummy = []\n",
    "    for col in dummy_list:\n",
    "        if (df[col].dtypes == 'float64' or df[col].dtypes == 'int64'):\n",
    "            row_dummy.append([col, df[col].dtypes, df[col].min(), df[col].max(), number_nan[col]])\n",
    "    df_dummy = pd.DataFrame(row_dummy,columns=['column_name', 'type', 'min', 'max', 'number NaN'])\n",
    "    display(df_dummy)\n",
    "    \n",
    "    print('___________________________________')\n",
    "    printmd('- There are ***{} numerical*** ({}x int and {}x float) columns'.format(count_int + count_float, count_int, count_float))\n",
    "    printmd('- There are ***{} categorical*** columns'.format(count_object))\n",
    "    printmd('- There are ***{} dummy*** columns'.format(len(dummy_list)))\n",
    "    \n",
    "    print('___________________________________')\n",
    "    printmd('- There are ***{} missing values*** in total in the dataset'.format(df.isnull().values.sum()))\n",
    "  \n",
    "get_dataframe_summary(df, cat_list=['key', 'mode', 'explicit'],dummy_list=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization will be needed!\n",
    "- As it can be seen from the min max ranges of all variables, a normalization (or standardization) approach may be beneficial in order to obtain fruitful predictions for popularity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The numerical part: Remove some variables\n",
    "As the focus is on song popularity one should remove variables that are not useful for its prediction\n",
    "- ***id:*** Splits into 170653 unique entires. This is the number of rows in the dataset. This column should be removed from the data investigation.\n",
    "- ***name:*** Can be removed as there is too less power for a popularity prediction. There are 133638 unique entries (78% of the dataframe).There are some dublicates, but too less, to justify its transformation to dummy variables.\n",
    "- ***release-date:*** as it can be already seen from rhe desciptive statistics table(min and max) there is some mixture in datetime formats. It can be ignored as the focus is on a yearly based analysis. This information is already provided in the 'year' column.\n",
    "\n",
    "#### Let's start with an investigation of the numerical values: So remove: \n",
    "- ***artists:*** For sure, this is an important column for popularity. By inuition it is clear, that for an already famous artist it will be more easy to produce a new popular song than for an unknown artist. However, in the first part it will be removed.\n",
    "- ***mode:*** This is a categorical variable with 0 and 1. Mode will be investigated separately.\n",
    "- ***key:*** A categorical variable with values ranging between 0 and 11, each value for one note. Key will be analyzed separately.\n",
    "- ***explicit*** is a categorical variable with 0 and 1. It's dependence on popularity will be studied separately.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_columns = df.drop(['id', 'name', 'release_date', 'artists', 'mode', 'key', 'explicit'], axis=1).columns.tolist()\n",
    "printmd('#### Columns to keep')\n",
    "print(x_columns)\n",
    "\n",
    "data_num = df.copy()\n",
    "data_num = data_num.drop(['id', 'name', 'release_date', 'artists', 'mode', 'key', 'explicit'], axis=1)\n",
    "printmd('#### New shape of data_num')\n",
    "print(data_num.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDFs and Scatter plots of numerical variables for popularity study: \n",
    "Let's start with the numerical variables of the dataset. A more focused analysis of categorical/dummy variables will be done later on. So let's study the ramaining columns of data_num a bit more intensively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "for i, xvar in enumerate(x_columns):\n",
    "    plt.subplot(3,4,i+1)\n",
    "    sns.scatterplot(x=df[xvar], y=df.popularity)\n",
    "    \n",
    "#plt.subplots_adjust(space = 0.5)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "count=1\n",
    "for xvar in x_columns:\n",
    "    plt.subplot(12,2, count);\n",
    "    count += 1\n",
    "    plt.gca().set_title(xvar + ' distribution 1921-2020');\n",
    "    sns.distplot(a=df[xvar], kde=False);\n",
    "    plt.subplot(12,2,count);\n",
    "    count +=1\n",
    "    plt.gca().set_title('Popularity depening on ' + xvar);\n",
    "    sns.scatterplot(x=df[xvar], y=df.popularity);\n",
    "\n",
    "#plt.subplots_adjust(space = 0.5)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PDF distributions (left) and Scatter plots (right)\n",
    "- The pdf plots (left side) help to understand the value distributions of all the variables.\n",
    "- The scatter plots (right side) show the dependence of popularity on the numerical variables\n",
    "\n",
    "### Overview of the numerical variables\n",
    "- danceability: The relative measurement of the track being danceable, (Ranges from 0 to 1)\n",
    "- energy: The energy of the track,  (Ranges from 0 to 1)\n",
    "- duration_ms: The length of the track in milliseconds (ms), (Integer typically ranging from 200k to 300k)\n",
    "- instrumentalness:, The relative ratio of the track being instrumental, (Ranges from 0 to 1)\n",
    "- valence: The positiveness of the track, (Ranges from 0 to 1)\n",
    "- popularity: The popularity of the song lately, default country = US, (Ranges from 0 to 100)\n",
    "- tempo:The tempo of the track in Beat Per Minute (BPM), (Float typically ranging from 50 to 150)\n",
    "- liveness: The relative duration of the track sounding as a live performance, (Ranges from 0 to 1)\n",
    "- loudness: Relative loudness of the track in decibel (dB), (Float typically ranging from -60 to 0)\n",
    "- speechiness: The relative length of the track containing any kind of human voice, (Ranges from 0 to 1)\n",
    "- year: The release year of track, (Ranges from 1921 to 2020)\n",
    "- id, The primary identifier for the track, generated by Spotify\n",
    "\n",
    "### Some first glance results:\n",
    "\n",
    "#### Why is 'year' so important for popularity?\n",
    "- It is obvious that popularity increases with years. There is a strong linear dependency between both. Unfortunately, There isn't a kind of ‚Amount of Advertisement‘ column or ‚Social-Media-Likes‘-Column in the dataset. So there is room for speculation. But it is obvious, that it could be related to the development of growing music fan triggered by growing (social) media platforms with the years.\n",
    "\n",
    "#### Outliers are dominant in some cases\n",
    "\n",
    "- For example: Outliers are obvious for ***<a href='#tempo'>tempo</a>*** with some 0bpm cases but a certain popularity. This has to be checked.\n",
    "- Furthermore ***<a href='#speechness'>speechness</a>*** and ***<a href='#liveness'>liveness</a>*** are showing some 0 values with a strange disjointed gap. \n",
    "- ***<a href='#acousticness'>acousticness</a>***, shows a binary tendency between 0 and 1. It seems that a kind of logistic descision/classification was used two separate between two possible scenarios, namely acoustic or non-acoustic songs.\n",
    "- ***<a href='#popularity'>popularity</a>*** shows a lot 0 values. It is not really clear if those values are true or due to missing data. Especially in the year is strange. It is the only year with almost the whole range of values. It could be that those values are related to ongoing data collection (not finished data setting for 2020)\n",
    "- Not all variables are normally distributed. Hence, in a strict Linear Regression Approach via OLS some data arrangement would be needed to obtain stable predictions. Some data cleaning could help to optimize predictions.\n",
    "\n",
    "Those phenomena will be studied in more detail in the next sections: 'Identifying quasi NaN values' and ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying 'quasi' NaN values\n",
    "There are some columns with zero values which need special attention:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in x_columns:\n",
    "    print('Proportion of 0 values in ', col,': ', round((len(df)- np.count_nonzero(df[col], axis=0))/len(df)*100,2), '%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing 'quasi' NaN values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='tempo'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tempo:\n",
    "There are some songs with 0bpm but with a real popularity. This observation must be checked. Are those real songs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_zero_tempo = df[df['tempo'] ==0].sort_values(by='popularity', ascending=False)\n",
    "printmd('***# of songs with 0 bpm:*** ' + str(data_zero_tempo.shape[0]))\n",
    "printmd('*** Examples for 0 bpm songs:***')\n",
    "display(data_zero_tempo[['name', 'artists', 'tempo', 'popularity', 'year']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Result - tempo*** In fact, those 'songs' with zero bpm are not really songs but noises or sounds. As the goal is to give an asnswer to 'How to write popular ***real*** songs', those data should be removed from the dataset.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's remove 0 values rows for tempo\n",
    "data_num = data_num[data_num['tempo'] !=0]\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(2,2,1)\n",
    "plt.gca().set_title('Uncleaned tempo distribution 1921-2020')\n",
    "sns.distplot(a=df.tempo, kde=False)\n",
    "plt.subplot(2,2,2)\n",
    "plt.gca().set_title('Uncleaned tempo depening on year')\n",
    "sns.scatterplot(x=df.tempo, y=df.popularity)\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.gca().set_title('Cleaned tempo distribution 1921-2020')\n",
    "sns.distplot(a=data_num.tempo, kde=False)\n",
    "plt.subplot(2,2,4)\n",
    "plt.gca().set_title('Cleaned tempo depening on year')\n",
    "sns.scatterplot(x=data_num.tempo, y=data_num.popularity)\n",
    "plt.subplots_adjust(hspace = 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results after cleaning:\n",
    "#### PDF (power ditribution function) of popularity:\n",
    "- The pdf of popularity is after the cleaning step much more normally ditributed than before the cleaning process. A better model prediction is expected from the cleaning step.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='speechness'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### speechness:\n",
    "Furthermore speechness shows some 0 values with a strange disjointed gap. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_zero_speechness = df[df['speechness'] ==0].sort_values(by='popularity', ascending=False)\n",
    "printmd('***# of songs with 0 speechness:*** ' + str(data_zero_speechness.shape[0]))\n",
    "printmd('*** Examples for 0 speechness songs:***')\n",
    "display(data_zero_tempo[['name', 'artists', 'tempo', 'popularity', 'year']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='liveness'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### liveness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='acousticness'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### acousticness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='popularity'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### popularity:\n",
    "There is an enourmous amount of 0 values for popularity between 1921 and 1950 and especially for 2020. For 2020 this observation could be related to no non completed datasets or real missing data. Hence it is worth to dare tests where those observations are treated as outliers to enhance a popularity prediction model. The amount of 0 for popularity results in 16%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='clean_pop'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's remove 0 values rows for popularity\n",
    "data_num = data_num[data_num['popularity'] !=0]\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(2,2,1)\n",
    "plt.gca().set_title('Uncleaned popularity distribution 1921-2020')\n",
    "sns.distplot(a=df.popularity, kde=False)\n",
    "plt.subplot(2,2,2)\n",
    "plt.gca().set_title('Uncleaned popularity depening on year')\n",
    "sns.scatterplot(x=df.year, y=df.popularity)\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.gca().set_title('Cleaned popularity distribution 1921-2020')\n",
    "sns.distplot(a=data_num.popularity, kde=False)\n",
    "plt.subplot(2,2,4)\n",
    "plt.gca().set_title('Cleaned popularity depening on year')\n",
    "sns.scatterplot(x=data_num.year, y=data_num.popularity)\n",
    "plt.subplots_adjust(hspace = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_set = data_cleaned.columns.tolist()\n",
    "display(column_set)\n",
    "display(len(column_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In order to write a popular song, is the key and mode important?\n",
    "Does popularity depend on the categorical values of key and/or mode?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributions of key and mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of musical notes for backward Key Note Translation\n",
    "key_dictionary = {0: 'C', 1:'C#', 2:'D', 3:'D#', 4:'E', 5:'F', 6:'F#', 7:'G', 8:'G#', 9:'A', 10:'A#', 11:'B'}\n",
    "key_list = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B'] \n",
    "mode_dictionary = {0: 'Minor', 1:'Major'}\n",
    "\n",
    "data_cleaned['key_note'] = data_cleaned['key'].map(key_dictionary) # Create a 'key_note' column\n",
    "data_cleaned['mode_trans'] = data_cleaned['mode'].map(mode_dictionary) # Create a 'mode_trans' column\n",
    "\n",
    "# Distribution of key notes\n",
    "df_key_mode_vals_all = pd.DataFrame(index=key_list)\n",
    "df_key_mode_vals_all['Major'] = data_cleaned[data_cleaned['mode_trans']=='Major']['key_note'].value_counts() #Provide a pandas series of the counts for each key note in Major\n",
    "df_key_mode_vals_all['Minor'] = data_cleaned[data_cleaned['mode_trans']=='Minor']['key_note'].value_counts() #Provide a pandas series of the counts for each key note in Minor\n",
    "#display(df_key_mode)\n",
    "plot_bar(df_key_mode_vals_all.sort_values(by='Major', ascending=False)/df.shape[0]*100, title ='Distribution of keys between 1921-2020', y_label = '[%]', width=0.8)\n",
    "        \n",
    "\n",
    "# Does the key note influence song popularity?\n",
    "df_key_mode_pop_all = pd.DataFrame()\n",
    "df_key_mode_pop_all['Major'] = data_cleaned[data_cleaned['mode_trans']=='Major'].groupby(['key_note']).mean()['popularity']\n",
    "df_key_mode_pop_all['Minor'] = data_cleaned[data_cleaned['mode_trans']=='Minor'].groupby(['key_note']).mean()['popularity']\n",
    "#display(df_keyMode_popularity)\n",
    "plot_bar(df_key_mode_pop_all, title ='Popularity triggered by key/mode (1921-2020)', y_label = '[%]', width=0.8)\n",
    "\n",
    "df_key_mode_vals_2011_2020 = pd.DataFrame()\n",
    "df_decade = data_cleaned[(data_cleaned['year'] >= 2011) & (data_cleaned['year'] <= 2020)]\n",
    "df_key_mode_vals_2011_2020['2011-2020_maj'] = df_decade[df_decade['mode_trans']=='Major']['key_note'].value_counts()/df_decade.shape[0]*100 #Provide a pandas series of the counts for each key note\n",
    "df_key_mode_vals_2011_2020['2011_2020_min'] = df_decade[df_decade['mode_trans']=='Minor']['key_note'].value_counts()/df_decade.shape[0]*100 #Provide a pandas series of the counts for each key note\n",
    "df_to_plot = df_key_mode_vals_2011_2020.loc[:,['2011-2020_maj', '2011_2020_min']].sort_values(by='2011-2020_maj', ascending=False)\n",
    "plot_bar(df_to_plot, title ='Distribution of keys between 2011-2020', y_label = '[%]', width=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_vals = data_cleaned['mode_trans'].value_counts() #Provide a pandas series of the counts for each key note\n",
    "#display(mode_vals)\n",
    "plot_pie(data_cleaned['mode_trans'].value_counts(), title ='Distribution of Mode in % (1921-2020)', explode=[0.1, 0])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does the mode influence song popularity?\n",
    "mode_popularity = data_cleaned.groupby(['mode_trans']).mean()['popularity']\n",
    "#display(mode_popularity)\n",
    "plot_pie(mode_popularity, title ='Popularity triggered by mode in % (1921-2020)', explode=[0.1, 0])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RESULTS - Key and Mode between 1921-2020:\n",
    "- More than 20% of all songs between 1921 and 2020 are written in the keys C Major or G Major.\n",
    "- More than 70% percent of the songs are written in Major mode\n",
    "- There is no clear trend that the key note would trigger the popularity of a song.\n",
    "- There is also no significant trend that the mode would trigger the popularity of a song\n",
    "\n",
    "***Takeaway message:*** \n",
    "The key and the mode of a song does not significantly inluence the popularity of a song.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The C# HipHop phenomenon in the decade 2011-2020\n",
    "Why are there so many songs in C# Major in the period 2011-2020 (especially between 2017 and 2020)?\n",
    "- There is a significant shift from C to C# in the frequency distribution. This fact could be related to several circumstances:\n",
    "    - Wrong classification. Via own sample investigation there are a lot of C# Major songs in the database which are misclassified. The database label for C# is 1. It seems that in a lot of cases, which were difficult to analyze, the label 1 was used.  \n",
    "    - In addition, accordingly to a post (https://vi-control.net/community/threads/why-flat-sharp-keys-are-so-usual-into-hip-hop.29561/) there seems to be an actual trend in HipHop to use flat keys: C# Major seems to be a favorite key. By an analysis over 500,000 Echo Nest tracks the following relations were reported. If flat keys are used than 20.0% are written in Db (C#).\n",
    "    - However: For HipHop based tracks the mode of the song is in some cases not clear. So some kind of misclassification is going on. Some proportion of C#  Minor songs are also misinterpreted as C# Major songs.\n",
    "    - Guitarists like to use a capodaster in the first fret. C Major Songs are transposed to C# Major songs in that way. A famous example is the song Lemon Tree from Fools Garden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hiphop = df.copy()\n",
    "df_hiphop['key_note'] = df_hiphop['key'].map(key_dictionary) # Create a 'key_note' column\n",
    "df_hiphop['mode_trans'] = df_hiphop['mode'].map(mode_dictionary) # Create a 'mode_trans' column\n",
    "\n",
    "name_list = ['Lemon Tree', 'WAP (feat. Megan Thee Stallion)', 'If the World Was Ending - feat. Julia Michaels', 'We Paid (feat. 42 Dugg)']\n",
    "printmd('### Some actual and/or popular C# major songs')\n",
    "df_hiphop[(df_hiphop['key_note'] == 'C#') & (df_hiphop['mode_trans'] == 'Major') & (df_hiphop['name'].isin(name_list))][['name', 'artists', 'popularity', 'year','key_note', 'mode_trans']].sort_values(by='popularity',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Popularity dependence on key and mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does the key note influence song popularity?\n",
    "df_keyMode_popularity = pd.DataFrame()\n",
    "df_keyMode_popularity['Major'] = data_cleaned[data_cleaned['mode_trans']=='Major'].groupby(['key_note']).mean()['popularity']\n",
    "df_keyMode_popularity['Minor'] = data_cleaned[data_cleaned['mode_trans']=='Minor'].groupby(['key_note']).mean()['popularity']\n",
    "#display(df_keyMode_popularity)\n",
    "\n",
    "plot_bar(df_keyMode_popularity, title ='Popularity triggered by key/mode (1921-2020)', width=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at correlations\n",
    "plt.figure(figsize = (15,15)) #creating the 'canvas'\n",
    "sns.heatmap(data_cleaned.corr(), annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort correaltion matrix\n",
    "correlation_mat = data_cleaned.corr()\n",
    "corr_pairs = correlation_mat.unstack().sort_values()\n",
    "corr_pop = corr_pairs['popularity'].drop('popularity', axis=0)\n",
    "#display(corr_pop)\n",
    "plot_bar(corr_pop, title ='Influence on song popularity (1921-2020)', width=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results for correlation\n",
    "#### Clear negative influence on song popularity for \n",
    "- acousticness: correlated also negatively with loudness, energy and danceability and year\n",
    "- instrumentalness: similar (but less signal) to acousticness correlates also negatively with loudness, energy and danceability and year)\n",
    "\n",
    "#### Clear positive influence on song popularity. e.g. for\n",
    "- year \n",
    "- energy\n",
    "- loudness\n",
    "- danceability\n",
    "\n",
    "The correlation with duration_ms, valence, key, mode and liveness is weak.\n",
    "Some kind (but minor) correlation can be observed for explicit, tempo - positiv - and for speechness - negativ.\n",
    "\n",
    "Why is Year so important?\n",
    "There is no kind of ‚Amount of Advertisement‘ column or ‚Social-Media-Likes‘-Column in the dataset. So there is room for speculation. But it is obvious, that it could be related to the development of growing music fan bases by growing (social) media platforms with the years. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dummy variables\n",
    "Now it is time to look closer to the categrorical variables like 'key'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DUMMY VARIABLES\n",
    "def create_dummy_df(df, cat_cols, dummy_na):\n",
    "    '''\n",
    "    INPUT:\n",
    "    df - pandas dataframe with categorical variables you want to dummy\n",
    "    cat_cols - list of strings that are associated with names of the categorical columns\n",
    "    dummy_na - Bool holding whether you want to dummy NA vals of categorical columns or not\n",
    "    \n",
    "    OUTPUT:\n",
    "    df - a new dataframe that has the following characteristics:\n",
    "            1. contains all columns that were not specified as categorical\n",
    "            2. removes all the original columns in cat_cols\n",
    "            3. dummy columns for each of the categorical columns in cat_cols\n",
    "            4. if dummy_na is True - it also contains dummy columns for the NaN values\n",
    "            5. Use a prefix of the column name with an underscore (_) for separating \n",
    "    '''\n",
    "    \n",
    "    cat_df = df.select_dtypes(include=['object']).copy() \n",
    "    #Create a copy of the dataframe\n",
    "    cat_df_copy = cat_df.copy()\n",
    "    #Pull a list of the column names of the categorical variables\n",
    "    cat_cols_lst = cat_df.columns\n",
    "    \n",
    "    for col in  cat_cols:\n",
    "        try:\n",
    "            # for each cat add dummy var, drop original column\n",
    "            df = pd.concat([df.drop(col, axis=1), pd.get_dummies(df[col], prefix=col, prefix_sep='_', drop_first=True, dummy_na=dummy_na)], axis=1)\n",
    "        except:\n",
    "            continue\n",
    "    return df\n",
    "\n",
    "cat_cols_lst = ['key']\n",
    "df_clean_cat = create_dummy_df(data_cleaned, cat_cols_lst, dummy_na=False) #Use your newly created function\n",
    "\n",
    "#display(df_clean_cat.head())\n",
    "printmd('#### New shape of df_cleaned')\n",
    "print(data_cleaned.shape)\n",
    "printmd('#### New shape of df_clean_cat')\n",
    "print(df_clean_cat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results after dummy implementation:\n",
    "- for each key (C, C#, D, D#, E, F, F#, G, G#, A, A#, B) a one hot vector was inserted in df_cleaned.\n",
    "- The number of columns for df_clean_cat increased to 29"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SKLEARN - ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_columns = data_cleaned.drop(['key_note', 'mode_trans'], axis=1).columns.tolist()\n",
    "print(x_columns)\n",
    "print(len(x_columns))\n",
    "\n",
    "df_for_ml = data_cleaned.copy()\n",
    "df_for_ml = data_cleaned.drop(['key_note', 'mode_trans'], axis=1)\n",
    "\n",
    "column_set = df_for_ml.columns.tolist()\n",
    "display(column_set)\n",
    "display(len(column_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdef clean_fit_linear_mod(df, response_col, cat_cols, dummy_na, test_size=.3, rand_state=42):\n",
    "    '''\n",
    "    INPUT:\n",
    "    df - a dataframe holding all the variables of interest\n",
    "    response_col - a string holding the name of the column \n",
    "    cat_cols - list of strings that are associated with names of the categorical columns\n",
    "    dummy_na - Bool holding whether you want to dummy NA vals of categorical columns or not\n",
    "    test_size - a float between [0,1] about what proportion of data should be in the test dataset\n",
    "    rand_state - an int that is provided as the random state for splitting the data into training and test \n",
    "    \n",
    "    OUTPUT:\n",
    "    test_score - float - r2 score on the test data\n",
    "    train_score - float - r2 score on the test data\n",
    "    lm_model - model object from sklearn\n",
    "    X_train, X_test, y_train, y_test - output from sklearn train test split used for optimal model\n",
    "    \n",
    "    Your function should:\n",
    "    1. Drop the rows with missing response values\n",
    "    2. Drop columns with NaN for all the values\n",
    "    3. Use create_dummy_df to dummy categorical columns\n",
    "    4. Fill the mean of the column for any missing values \n",
    "    5. Split your data into an X matrix and a response vector y\n",
    "    6. Create training and test sets of data\n",
    "    7. Instantiate a LinearRegression model with normalized data\n",
    "    8. Fit your model to the training data\n",
    "    9. Predict the response for the training data and the test data\n",
    "    10. Obtain an rsquared value for both the training and test data\n",
    "    '''\n",
    "    #Drop the rows with missing response values\n",
    "    df  = df.dropna(subset=[response_col], axis=0)\n",
    "\n",
    "    #Drop columns with all NaN values\n",
    "    df = df.dropna(how='all', axis=1)\n",
    "\n",
    "    #Dummy categorical variables\n",
    "    df = create_dummy_df(df, cat_cols, dummy_na)\n",
    "\n",
    "    # Mean function\n",
    "    fill_mean = lambda col: col.fillna(col.mean())\n",
    "    # Fill the mean\n",
    "    df = df.apply(fill_mean, axis=0)\n",
    "\n",
    "    #Split into explanatory and response variables\n",
    "    X = df.drop(response_col, axis=1)\n",
    "    y = df[response_col]\n",
    "\n",
    "    #Split into train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=rand_state)\n",
    "\n",
    "    lm_model = LinearRegression(normalize=True) # Instantiate\n",
    "    lm_model.fit(X_train, y_train) #Fit\n",
    "\n",
    "    #Predict using your model\n",
    "    y_test_preds = lm_model.predict(X_test)\n",
    "    y_train_preds = lm_model.predict(X_train)\n",
    "\n",
    "    #Score using your model\n",
    "    test_score = r2_score(y_test, y_test_preds)\n",
    "    train_score = r2_score(y_train, y_train_preds)\n",
    "\n",
    "    return test_score, train_score, lm_model, X_train, X_test, y_train, y_test\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "response_col = 'popularity'\n",
    "cat_cols_lst = ['key']\n",
    "\n",
    "#Test your function with the above dataset\n",
    "test_score, train_score, lm_model, X_train, X_test, y_train, y_test = clean_fit_linear_mod(df_for_ml, response_col, cat_cols_lst, dummy_na=False)\n",
    "\n",
    "#Print training and testing score\n",
    "print(\"The rsquared on the training data was {}.  The rsquared on the test data was {}.\".format(train_score, test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning approach with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data in train, validation and test set like 60:20:20\n",
    "train, valid, test = np.split(df_for_ml.sample(frac=1, random_state=42),[int(.6*len(data)), int(.8*len(data))])\n",
    "\n",
    "#rescaling of data\n",
    "max_ = train.max(axis=0)\n",
    "min_ = train.min(axis=0)\n",
    "delta_ = max_ - min_\n",
    "train = (train - min_) / delta_\n",
    "valid = (valid - min_) / delta_\n",
    "test = (test - min_) / delta_\n",
    "\n",
    "#prepare data for DL\n",
    "y_train = train.danceability\n",
    "X_train = train.drop([''], axis=1)\n",
    "y_valid = train.danceability\n",
    "X_valid = train.drop(['popularity'], axis=1)\n",
    "y_test = train.danceability\n",
    "X_test = train.drop(['popularity'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(\n",
    "    min_delta=0.001, # minimium amount of change to count as an improvement\n",
    "    patience=10, # how many epochs to wait before stopping\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "dl_model = keras.Sequential([\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(20, activation='relu', input_shape=[df_for_ml.shape[1]]),\n",
    "    layers.Dropout(rate=0.3),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(20, activation='relu'),\n",
    "    layers.Dropout(rate=0.3),\n",
    "    layers.Dense(1),\n",
    "])\n",
    "\n",
    "dl_model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"mae\",\n",
    ")\n",
    "\n",
    "history = dl_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    batch_size=265,\n",
    "    epochs=100,\n",
    "    callbacks=[early_stopping], # put your callbacks in a list\n",
    "    verbose=0,  # turn off training log\n",
    ")\n",
    "\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.loc[:, ['loss', 'val_loss']].plot();\n",
    "print(\"Minimum validation loss: {}\".format(history_df['val_loss'].min()))"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
